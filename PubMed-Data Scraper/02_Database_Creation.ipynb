{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 # PostgreSQL database adapter for the Python programming language\n",
    "from psycopg2 import sql # SQL module for psycopg2 that supports advanced functionality and datatypes\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import getpass # Import the getpass module to hide the password input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where output Parquet files will be stored.\n",
    "directory = r\"\"\n",
    "\n",
    "# Define PostgreSQL connection details\n",
    "db_config = {\n",
    "    \"dbname\": getpass.getpass(\"Enter your database name: \"),\n",
    "    \"user\" : getpass.getpass(\"Enter your PostgreSQL username: \"),\n",
    "    \"password\": getpass.getpass(\"Enter your PostgreSQL password: \"),\n",
    "    \"host\": getpass.getpass(\"Enter your PostgreSQL host name: \"),  \n",
    "    \"port\":  getpass.getpass(\"Enter your PostgreSQL PostgreSQL port number: \")\n",
    "}\n",
    "\n",
    "# Establish a connection\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    print(\"Connected to PostgreSQL successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop an existing database and create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping existing tables...\n",
      "Creating tables...\n",
      "Database schema updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Drop & Create Tables\n",
    "if conn:\n",
    "    try:\n",
    "        with conn.cursor() as cursor: # Create a cursor object to execute queries. Cursor is a control structure used to traverse and fetch records from the database.\n",
    "            print(\"Dropping existing tables...\")\n",
    "            cursor.execute('DROP TABLE IF EXISTS bridge_articles_search_terms CASCADE;') # CASCADE is used to drop all dependent objects.\n",
    "            cursor.execute('DROP TABLE IF EXISTS bridge_articles_authors CASCADE;')\n",
    "            cursor.execute('DROP TABLE IF EXISTS fact_articles CASCADE;')\n",
    "            cursor.execute('DROP TABLE IF EXISTS dim_authors CASCADE;')\n",
    "            cursor.execute('DROP TABLE IF EXISTS dim_search_terms CASCADE;')\n",
    "            cursor.execute('DROP TABLE IF EXISTS dim_journals CASCADE;')\n",
    "            cursor.execute('DROP TABLE IF EXISTS dim_titles CASCADE;')\n",
    "\n",
    "            print(\"Creating tables...\")\n",
    "\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE dim_titles (\n",
    "                    title_id SERIAL PRIMARY KEY,\n",
    "                    title TEXT UNIQUE NOT NULL\n",
    "                );\n",
    "            ''')\n",
    "\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE dim_journals (\n",
    "                    journal_id SERIAL PRIMARY KEY,\n",
    "                    journal_name TEXT UNIQUE NOT NULL\n",
    "                );\n",
    "            ''')\n",
    "\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE dim_search_terms (\n",
    "                    search_term_id SERIAL PRIMARY KEY,\n",
    "                    search_term TEXT UNIQUE NOT NULL\n",
    "                );\n",
    "            ''')\n",
    "\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE dim_authors (\n",
    "                    author_id SERIAL PRIMARY KEY,\n",
    "                    author_name TEXT UNIQUE NOT NULL\n",
    "                );\n",
    "            ''')\n",
    "\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE fact_articles (\n",
    "                    article_id SERIAL PRIMARY KEY,\n",
    "                    pmid TEXT UNIQUE NOT NULL,\n",
    "                    publication_date DATE,\n",
    "                    title_id INTEGER REFERENCES dim_titles(title_id),\n",
    "                    journal_id INTEGER REFERENCES dim_journals(journal_id)\n",
    "                );\n",
    "            ''')\n",
    "\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE bridge_articles_authors (\n",
    "                    article_id INTEGER REFERENCES fact_articles(article_id) ON DELETE CASCADE,\n",
    "                    author_id INTEGER REFERENCES dim_authors(author_id) ON DELETE CASCADE,\n",
    "                    PRIMARY KEY (article_id, author_id)\n",
    "                );\n",
    "            ''')\n",
    "\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE bridge_articles_search_terms (\n",
    "                    article_id INTEGER REFERENCES fact_articles(article_id) ON DELETE CASCADE,\n",
    "                    search_term_id INTEGER REFERENCES dim_search_terms(search_term_id) ON DELETE CASCADE,\n",
    "                    PRIMARY KEY (article_id, search_term_id)\n",
    "                );\n",
    "            ''')\n",
    "\n",
    "            conn.commit()\n",
    "            print(\"Database schema updated successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating database schema: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the newest Parquet file in the directory\n",
    "try:\n",
    "    files = [file for file in glob.glob(os.path.join(directory, \"*.parquet\"))] # Get a list of all Parquet files in the directory. *.parquet is a wildcard that matches all files with the .parquet extension.\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No valid Parquet files found in the directory.\")\n",
    "\n",
    "    newest_file = max(files, key=os.path.getmtime) # Get the newest file based on last modified time\n",
    "    print(f\"Newest Parquet file detected: {newest_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error finding the newest file: {e}\")\n",
    "    newest_file = None\n",
    "\n",
    "# Load the newest Parquet file into a Pandas DataFrame\n",
    "if newest_file:\n",
    "    try:\n",
    "        df_input = pd.read_parquet(newest_file)\n",
    "        print(\"Parquet file loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Parquet file: {e}\")\n",
    "        df_input = None\n",
    "\n",
    "# Ensure correct data types\n",
    "if df_input is not None:\n",
    "    df_input['PMID'] = df_input['PMID'].astype(str)  # Ensure PMID is string type\n",
    "    df_input.fillna(\"Unknown\", inplace=True)  # Fill missing values\n",
    "\n",
    "# Insert Data into PostgreSQL\n",
    "if df_input is not None:\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config) # Reconnect to ensure connection is still open. **db_config unpacks the dictionary into keyword arguments for the connect method of psycopg2 module. \n",
    "        with conn.cursor() as cursor: # Use the cursor object to interact with the database\n",
    "            print(\"Inserting data into tables...\")\n",
    "\n",
    "            # Insert into dim_titles table. ON CONFLICT DO NOTHING prevents duplicate entries. \n",
    "            # %s is a placeholder for the values that will be inserted into the table.\n",
    "            for title in df_input['Title'].unique():\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO dim_titles (title)\n",
    "                    VALUES (%s)\n",
    "                    ON CONFLICT (title) DO NOTHING;\n",
    "                ''', (title,))\n",
    "\n",
    "            # Insert into dim_journals\n",
    "            for journal in df_input['Journal'].unique():\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO dim_journals (journal_name)\n",
    "                    VALUES (%s)\n",
    "                    ON CONFLICT (journal_name) DO NOTHING;\n",
    "                ''', (journal,))\n",
    "\n",
    "            # Insert into dim_search_terms\n",
    "            for term in df_input['Search Term'].unique():\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO dim_search_terms (search_term)\n",
    "                    VALUES (%s)\n",
    "                    ON CONFLICT (search_term) DO NOTHING;\n",
    "                ''', (term,))\n",
    "\n",
    "            # Insert into dim_authors\n",
    "            for author in df_input['Author'].unique():\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO dim_authors (author_name)\n",
    "                    VALUES (%s)\n",
    "                    ON CONFLICT (author_name) DO NOTHING;\n",
    "                ''', (author,))\n",
    "\n",
    "            # Insert into fact_articles table with the retrieved IDs from the dim_titles and dim_journals tables. ON CONFLICT DO NOTHING prevents duplicate entries.\n",
    "            for _, row in df_input.iterrows(): # Iterates over each row of the DataFrame containing article data. df_input.iterrows() returns (index, row). The _ is a placeholder for the index of the row, because the index in not used for SQL insertion.\n",
    "                \n",
    "                cursor.execute('SELECT journal_id FROM dim_journals WHERE journal_name = %s;', (row['Journal'],)) # Retrieve the journal_id from the dim_journals table.\n",
    "                journal_id = cursor.fetchone()[0] # Fetch the first result from the query. Fetchone() retrieves the next row of a query result set, returning a tuple. [0] extracts the ID from the returned tuple\n",
    "\n",
    "                cursor.execute('SELECT title_id FROM dim_titles WHERE title = %s;', (row['Title'],))\n",
    "                title_id = cursor.fetchone()[0]\n",
    "\n",
    "                # Insert into fact_articles table with the retrieved IDs from the dim_titles and dim_journals tables.\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO fact_articles (pmid, title_id, publication_date, journal_id)\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    ON CONFLICT (pmid) DO NOTHING;\n",
    "                ''', (row['PMID'], title_id, row['Publication Date'], journal_id))\n",
    "\n",
    "                cursor.execute('SELECT article_id FROM fact_articles WHERE pmid = %s;', (row['PMID'],))\n",
    "                article_id = cursor.fetchone()[0]\n",
    "\n",
    "                # Insert into bridge_articles_authors table with the retrieved author_id from the dim_authors table.\n",
    "                cursor.execute('SELECT author_id FROM dim_authors WHERE author_name = %s;', (row['Author'],))\n",
    "                author_id = cursor.fetchone()[0]\n",
    "\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO bridge_articles_authors (article_id, author_id)\n",
    "                    VALUES (%s, %s)\n",
    "                    ON CONFLICT DO NOTHING;\n",
    "                ''', (article_id, author_id))\n",
    "\n",
    "                # Insert into bridge_articles_search_terms table with the retrieved search_term_id from the dim_search_terms table.\n",
    "                cursor.execute('SELECT search_term_id FROM dim_search_terms WHERE search_term = %s;', (row['Search Term'],))\n",
    "                search_term_id = cursor.fetchone()[0]\n",
    "\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO bridge_articles_search_terms (article_id, search_term_id)\n",
    "                    VALUES (%s, %s)\n",
    "                    ON CONFLICT DO NOTHING;\n",
    "                ''', (article_id, search_term_id))\n",
    "\n",
    "            conn.commit() # Commit the transaction and save the changes to the database.\n",
    "            print(\"Data inserted successfully into PostgreSQL!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data: {e}\")\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "#Close the Connection:\n",
    "if conn:\n",
    "    conn.close()\n",
    "    print(\"PostgreSQL connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the file location\n",
    "import os\n",
    "print(\"Database location:\", os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
